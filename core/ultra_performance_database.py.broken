"""
🚀 ULTRA-HIGH PERFORMANCE DATABASE & CACHING SYSTEM
Optimized data persistence with advanced caching strategies

Features:
- Multi-tier caching system
- Optimized database operations
- Memory management and pooling
- Async/await throughout
- Connection pooling
- Query optimization
- Data compression
- Real-time performance monitoring

Author: x1ziad
Version: 2.0.0 PERFORMANCE
"""

import asyncio
import logging
import time
import sqlite3
import aiosqlite
from typing import Dict, List, Optional, Any, Union, Tuple
from datetime import datetime, timezone, timedelta
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from collections import deque
import threading
import weakref
import hashlib
import gzip

# Performance imports with fallbacks
try:
    import orjson as fast_json

    USE_FAST_JSON = True
except ImportError:
    import json as fast_json

    USE_FAST_JSON = False

try:
    import lz4.frame as lz4

    USE_LZ4_COMPRESSION = True
except ImportError:
    USE_LZ4_COMPRESSION = False

logger = logging.getLogger("astra.database")


class CacheLevel(Enum):
    """Cache tier levels"""

    L1_MEMORY = 1  # In-memory, fastest
    L2_COMPRESSED = 2  # Compressed in-memory
    L3_DISK = 3  # On-disk cache
    DATABASE = 4  # Database storage


class DataType(Enum):
    """Data types for optimization"""

    PERSONALITY = "personality"
    SESSION = "session"
    USER_PROFILE = "user_profile"
    GUILD_CONFIG = "guild_config"
    ANALYTICS = "analytics"
    TEMPORARY = "temporary"


@dataclass
class CacheEntry:
    """Cache entry with metadata"""

    key: str
    value: Any
    created_at: datetime
    expires_at: Optional[datetime]
    access_count: int = 0
    last_accessed: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    size_bytes: int = 0
    compressed: bool = False
    data_type: DataType = DataType.TEMPORARY


@dataclass
class DatabaseMetrics:
    """Database performance metrics"""

    queries_executed: int = 0
    cache_hits: int = 0
    cache_misses: int = 0
    total_data_size: int = 0
    connections_active: int = 0
    avg_query_time: float = 0.0
    compression_ratio: float = 0.0
    memory_usage_mb: float = 0.0


class UltraPerformanceDatabase:
    """
    🚀 ULTRA-HIGH PERFORMANCE DATABASE SYSTEM

    Multi-tier caching with intelligent data management:
    - L1: Hot data in memory (< 1ms access)
    - L2: Compressed warm data (< 5ms access)
    - L3: Disk-cached data (< 50ms access)
    - DB: Persistent storage (< 200ms access)

    Optimizations:
    - Connection pooling
    - Query batching
    - Data compression
    - Smart prefetching
    - Automatic cleanup
    """

    def __init__(self, db_path: str = "data/astra_optimized.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        self.logger = logging.getLogger("astra.database")

        # Connection management
        self._connection_pool: List[aiosqlite.Connection] = []
        self._pool_size = 10
        self._pool_lock = asyncio.Lock()
        self._active_connections = 0

        # Multi-tier cache system
        self._l1_cache: Dict[str, CacheEntry] = {}  # Hot data
        self._l2_cache: Dict[str, bytes] = {}  # Compressed data
        self._l3_cache_path = self.db_path.parent / "cache"
        self._l3_cache_path.mkdir(exist_ok=True)

        # Cache configuration
        self._l1_max_size = 1000  # Max entries in L1
        self._l1_max_memory_mb = 100  # Max memory for L1
        self._l2_max_size = 5000  # Max entries in L2
        self._l3_max_size_mb = 500  # Max disk space for L3

        # Performance tracking
        self.metrics = DatabaseMetrics()
        self._query_times: List[float] = []

        # Data type configurations
        self._data_type_configs = {
            DataType.PERSONALITY: {
                "ttl_seconds": 1800,  # 30 minutes
                "compress": True,
                "cache_level": CacheLevel.L1_MEMORY,
            },
            DataType.SESSION: {
                "ttl_seconds": 900,  # 15 minutes
                "compress": True,
                "cache_level": CacheLevel.L2_COMPRESSED,
            },
            DataType.USER_PROFILE: {
                "ttl_seconds": 3600,  # 1 hour
                "compress": False,
                "cache_level": CacheLevel.L1_MEMORY,
            },
            DataType.GUILD_CONFIG: {
                "ttl_seconds": 7200,  # 2 hours
                "compress": False,
                "cache_level": CacheLevel.L1_MEMORY,
            },
            DataType.ANALYTICS: {
                "ttl_seconds": 300,  # 5 minutes
                "compress": True,
                "cache_level": CacheLevel.L3_DISK,
            },
            DataType.TEMPORARY: {
                "ttl_seconds": 300,  # 5 minutes
                "compress": False,
                "cache_level": CacheLevel.L1_MEMORY,
            },
        }

        # Background task handles
        self._cleanup_task: Optional[asyncio.Task] = None
        self._metrics_task: Optional[asyncio.Task] = None
        self._initialized = False

        self.logger.info("🚀 Ultra-Performance Database initialized")

    async def initialize(self):
        """Initialize the database system"""
        try:
            self.logger.info("🔧 Initializing Ultra-Performance Database...")

            # Setup database schema
            await self._setup_database()

            # Initialize connection pool
            await self._initialize_connection_pool()

            # Start background tasks
            self._start_background_tasks()

            # Load frequently accessed data
            await self._preload_hot_data()

            self._initialized = True
            self.logger.info("✅ Ultra-Performance Database ready")

        except Exception as e:
            self.logger.error(f"❌ Database initialization failed: {e}")
            raise

    async def _setup_database(self):
        """Setup optimized database schema"""
        async with aiosqlite.connect(self.db_path) as conn:
            # Performance optimizations
            await conn.execute("PRAGMA journal_mode=WAL")
            await conn.execute("PRAGMA synchronous=NORMAL")
            await conn.execute("PRAGMA cache_size=20000")
            await conn.execute("PRAGMA temp_store=MEMORY")
            await conn.execute("PRAGMA mmap_size=268435456")  # 256MB

            # Optimized tables with partitioning support
            await conn.execute(
                """
                CREATE TABLE IF NOT EXISTS data_store (
                    key TEXT PRIMARY KEY,
                    value BLOB,
                    data_type TEXT,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    expires_at TIMESTAMP,
                    access_count INTEGER DEFAULT 0,
                    compressed BOOLEAN DEFAULT FALSE,
                    size_bytes INTEGER DEFAULT 0
                )
            """
            )

            # Performance indexes
            await conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_data_type ON data_store(data_type)"
            )
            await conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_expires_at ON data_store(expires_at)"
            )
            await conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_updated_at ON data_store(updated_at)"
            )

            # Partitioned tables for high-volume data
            await conn.execute(
                """
                CREATE TABLE IF NOT EXISTS analytics_data (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    guild_id INTEGER,
                    data BLOB,
                    timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    partition_date TEXT GENERATED ALWAYS AS (date(timestamp)) STORED
                )
            """
            )

            await conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_analytics_partition ON analytics_data(partition_date)"
            )
            await conn.execute(
                "CREATE INDEX IF NOT EXISTS idx_analytics_guild ON analytics_data(guild_id)"
            )

            # Query optimization table
            await conn.execute(
                """
                CREATE TABLE IF NOT EXISTS query_cache (
                    query_hash TEXT PRIMARY KEY,
                    result BLOB,
                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                    expires_at TIMESTAMP
                )
            """
            )

            await conn.commit()

    async def _initialize_connection_pool(self):
        """Initialize connection pool for optimal performance"""
        async with self._pool_lock:
            for _ in range(self._pool_size):
                conn = await aiosqlite.connect(self.db_path)
                # Optimize each connection
                await conn.execute("PRAGMA journal_mode=WAL")
                await conn.execute("PRAGMA synchronous=NORMAL")
                await conn.execute("PRAGMA cache_size=10000")
                self._connection_pool.append(conn)

        self.logger.info(
            f"💾 Connection pool initialized with {self._pool_size} connections"
        )

    async def _get_connection(self) -> aiosqlite.Connection:
        """Get connection from pool"""
        async with self._pool_lock:
            if self._connection_pool:
                conn = self._connection_pool.pop()
                self._active_connections += 1
                return conn
            else:
                # Create new connection if pool empty
                conn = await aiosqlite.connect(self.db_path)
                await conn.execute("PRAGMA journal_mode=WAL")
                await conn.execute("PRAGMA synchronous=NORMAL")
                self._active_connections += 1
                return conn

    async def _return_connection(self, conn: aiosqlite.Connection):
        """Return connection to pool"""
        async with self._pool_lock:
            if len(self._connection_pool) < self._pool_size:
                self._connection_pool.append(conn)
            else:
                await conn.close()
            self._active_connections -= 1

    def _start_background_tasks(self):
        """Start optimized background maintenance tasks"""
        self._cleanup_task = asyncio.create_task(self._cache_cleanup_task())
        self._metrics_task = asyncio.create_task(self._metrics_collection_task())

        self.logger.info("🔄 Background maintenance tasks started")

    async def _cache_cleanup_task(self):
        """Intelligent cache cleanup with LRU eviction"""
        while self._initialized:
            try:
                await asyncio.sleep(60)  # Every minute

                now = datetime.now(timezone.utc)

                # L1 Cache cleanup (memory pressure based)
                await self._cleanup_l1_cache(now)

                # L2 Cache cleanup
                await self._cleanup_l2_cache(now)

                # L3 Cache cleanup (disk space management)
                await self._cleanup_l3_cache()

                # Database cleanup (expired records)
                await self._cleanup_database(now)

            except Exception as e:
                self.logger.error(f"Cache cleanup error: {e}")

    async def _cleanup_l1_cache(self, now: datetime):
        """Clean up L1 cache with smart eviction"""
        # Remove expired entries
        expired_keys = [
            key
            for key, entry in self._l1_cache.items()
            if entry.expires_at and now > entry.expires_at
        ]

        for key in expired_keys:
            del self._l1_cache[key]

        # Memory pressure eviction
        current_memory = sum(entry.size_bytes for entry in self._l1_cache.values()) / (
            1024 * 1024
        )

        if current_memory > self._l1_max_memory_mb:
            # Sort by access pattern (LRU with frequency consideration)
            sorted_entries = sorted(
                self._l1_cache.items(),
                key=lambda x: (
                    x[1].access_count / max(1, (now - x[1].created_at).total_seconds()),
                    x[1].last_accessed,
                ),
                reverse=False,  # Least valuable first
            )

            # Remove least valuable entries
            target_size = self._l1_max_memory_mb * 0.8  # Target 80% of max
            current_size = current_memory

            for key, entry in sorted_entries:
                if current_size <= target_size:
                    break

                # Move to L2 if suitable
                if self._data_type_configs[entry.data_type]["compress"]:
                    await self._move_to_l2_cache(key, entry)

                del self._l1_cache[key]
                current_size -= entry.size_bytes / (1024 * 1024)

        if expired_keys:
            self.logger.debug(f"🧹 L1 cleanup: {len(expired_keys)} expired entries")

    async def _cleanup_l2_cache(self, now: datetime):
        """Clean up L2 compressed cache"""
        # Simple size-based cleanup for L2
        if len(self._l2_cache) > self._l2_max_size:
            # Remove oldest entries (simple FIFO for L2)
            excess = len(self._l2_cache) - int(self._l2_max_size * 0.8)
            keys_to_remove = list(self._l2_cache.keys())[:excess]

            for key in keys_to_remove:
                del self._l2_cache[key]

            if keys_to_remove:
                self.logger.debug(f"🧹 L2 cleanup: {len(keys_to_remove)} entries")

    async def _cleanup_l3_cache(self):
        """Clean up L3 disk cache"""
        try:
            cache_files = list(self._l3_cache_path.glob("*.cache"))
            total_size = sum(f.stat().st_size for f in cache_files) / (1024 * 1024)

            if total_size > self._l3_max_size_mb:
                # Sort by last access time
                cache_files.sort(key=lambda f: f.stat().st_atime)

                target_size = self._l3_max_size_mb * 0.8
                current_size = total_size
                removed_count = 0

                for cache_file in cache_files:
                    if current_size <= target_size:
                        break

                    file_size = cache_file.stat().st_size / (1024 * 1024)
                    cache_file.unlink()
                    current_size -= file_size
                    removed_count += 1

                if removed_count > 0:
                    self.logger.debug(
                        f"🧹 L3 cleanup: {removed_count} files, {total_size-current_size:.1f}MB freed"
                    )

        except Exception as e:
            self.logger.error(f"L3 cleanup error: {e}")

    async def _cleanup_database(self, now: datetime):
        """Clean up expired database records"""
        try:
            conn = await self._get_connection()
            try:
                # Remove expired records
                cursor = await conn.execute(
                    "DELETE FROM data_store WHERE expires_at IS NOT NULL AND expires_at < ?",
                    (now.isoformat(),),
                )
                deleted_count = cursor.rowcount

                # Clean old analytics data (keep last 30 days)
                cutoff_date = (now - timedelta(days=30)).date().isoformat()
                cursor = await conn.execute(
                    "DELETE FROM analytics_data WHERE partition_date < ?",
                    (cutoff_date,),
                )
                analytics_deleted = cursor.rowcount

                # Clean query cache
                cursor = await conn.execute(
                    "DELETE FROM query_cache WHERE expires_at < ?", (now.isoformat(),)
                )
                cache_deleted = cursor.rowcount

                await conn.commit()

                if deleted_count > 0 or analytics_deleted > 0 or cache_deleted > 0:
                    self.logger.debug(
                        f"🧹 DB cleanup: {deleted_count} data, {analytics_deleted} analytics, {cache_deleted} cache"
                    )

            finally:
                await self._return_connection(conn)

        except Exception as e:
            self.logger.error(f"Database cleanup error: {e}")

    async def _metrics_collection_task(self):
        """🚀 ULTIMATE metrics collection and optimization"""
        while self._initialized:
            try:
                await asyncio.sleep(180)  # 🚀 More frequent - every 3 minutes

                # Calculate enhanced metrics
                total_ops = self.metrics.cache_hits + self.metrics.cache_misses
                hit_rate = (self.metrics.cache_hits / max(total_ops, 1)) * 100

                l1_size = len(self._l1_cache)
                l2_size = len(self._l2_cache)
                memory_usage = sum(
                    entry.size_bytes for entry in self._l1_cache.values()
                ) / (1024 * 1024)

                # 🚀 Real-time optimization based on metrics
                await self._real_time_optimization(hit_rate, memory_usage, l1_size, l2_size)

                # 🚀 Predictive cache management
                await self._predictive_cache_management(total_ops)

            except Exception as e:
                self.logger.error(f"Metrics collection error: {e}")

    async def _real_time_optimization(self, hit_rate: float, memory_usage: float, l1_size: int, l2_size: int):
        """🚀 Real-time database optimization"""
        try:
            # Optimize based on hit rate
            if hit_rate < 70:  # Poor hit rate
                # Increase L1 cache size if memory allows
                if memory_usage < 100:  # Less than 100MB
                    self._l1_max_size = min(self._l1_max_size * 1.1, 20000)
                    self.logger.info(f"🔄 Increased L1 cache size to {self._l1_max_size} for better hit rate")
                
                # Increase TTL for better retention
                self._default_ttl = min(self._default_ttl * 1.1, 7200)  # Max 2 hours
                
            elif hit_rate > 95:  # Excellent hit rate
                # Can optimize memory usage
                if memory_usage > 50:  # More than 50MB
                    await self._optimize_cache_memory()

            # Memory pressure optimization
            if memory_usage > 150:  # More than 150MB
                await self._emergency_cache_cleanup()

            # Connection pool optimization
            await self._optimize_connection_pool()

        except Exception as e:
            self.logger.error(f"Real-time optimization error: {e}")

    async def _optimize_cache_memory(self):
        """🚀 Optimize cache memory usage"""
        try:
            # Compress older L1 entries to L2
            now = datetime.now(timezone.utc)
            l1_keys = list(self._l1_cache.keys())
            
            # Find entries older than 5 minutes
            old_entries = []
            for key in l1_keys:
                entry = self._l1_cache[key]
                if (now - entry.created_at).total_seconds() > 300:  # 5 minutes
                    old_entries.append(key)
            
            # Move half of old entries to L2
            if old_entries:
                move_count = len(old_entries) // 2
                for key in old_entries[:move_count]:
                    entry = self._l1_cache.pop(key, None)
                    if entry and len(self._l2_cache) < self._l2_max_size:
                        # Compress and move to L2
                        try:
                            compressed_data = self._compress_data(entry.data)
                            self._l2_cache[key] = compressed_data
                        except:
                            pass  # Skip if compression fails
                
                self.logger.info(f"🔄 Moved {move_count} entries from L1 to L2 for memory optimization")

        except Exception as e:
            self.logger.error(f"Cache memory optimization error: {e}")

    async def _emergency_cache_cleanup(self):
        """🚀 Emergency cache cleanup for memory pressure"""
        try:
            self.logger.warning("🚨 High memory usage - performing emergency cache cleanup")
            
            # Aggressive L1 cleanup - keep only 50% of entries
            l1_keys = list(self._l1_cache.keys())
            keep_count = len(l1_keys) // 2
            
            # Sort by access time, keep most recently accessed
            sorted_keys = sorted(l1_keys, key=lambda k: self._l1_cache[k].last_accessed, reverse=True)
            
            removed_count = 0
            for key in sorted_keys[keep_count:]:
                if key in self._l1_cache:
                    del self._l1_cache[key]
                    removed_count += 1
            
            # Also cleanup L2 cache
            l2_keys = list(self._l2_cache.keys())
            if len(l2_keys) > self._l2_max_size // 2:
                remove_count = len(l2_keys) // 4  # Remove 25%
                for key in l2_keys[:remove_count]:
                    del self._l2_cache[key]
            
            self.logger.info(f"🧹 Emergency cleanup: removed {removed_count} L1 entries, {remove_count} L2 entries")
            
        except Exception as e:
            self.logger.error(f"Emergency cache cleanup error: {e}")

    async def _optimize_connection_pool(self):
        """🚀 Optimize database connection pool"""
        try:
            active_connections = len([conn for conn in self._connection_pool if not conn._in_pool])
            pool_utilization = active_connections / self._pool_size
            
            # Log pool status
            if pool_utilization > 0.8:
                self.logger.info(f"⚠️ High DB pool utilization: {pool_utilization:.1%}")
                
                # Could increase pool size if needed (within limits)
                if self._pool_size < 20:
                    await self._expand_connection_pool()
            
            elif pool_utilization < 0.2 and self._pool_size > 5:
                # Could shrink pool to save resources
                self.logger.debug("🔄 Low DB pool utilization - could optimize")

        except Exception as e:
            self.logger.error(f"Connection pool optimization error: {e}")

    async def _expand_connection_pool(self):
        """🚀 Expand connection pool for better performance"""
        try:
            new_size = min(self._pool_size + 2, 20)  # Add 2 connections, max 20
            
            # Create additional connections
            for _ in range(new_size - self._pool_size):
                conn = await aiosqlite.connect(self._db_path)
                await conn.execute("PRAGMA journal_mode=WAL")
                await conn.execute("PRAGMA synchronous=NORMAL")
                await conn.execute("PRAGMA cache_size=10000")
                await conn.execute("PRAGMA temp_store=MEMORY")
                setattr(conn, '_in_pool', True)
                self._connection_pool.append(conn)
            
            self._pool_size = new_size
            self.logger.info(f"🚀 Expanded connection pool to {new_size} connections")
            
        except Exception as e:
            self.logger.error(f"Connection pool expansion error: {e}")

    async def _predictive_cache_management(self, total_ops: int):
        """🚀 Predictive cache management"""
        try:
            # Track operation trends
            if not hasattr(self, '_ops_history'):
                self._ops_history = deque(maxlen=10)
            
            self._ops_history.append(total_ops)
            
            if len(self._ops_history) >= 5:
                # Calculate trend
                recent_ops = list(self._ops_history)[-5:]
                if len(recent_ops) >= 2:
                    ops_trend = (recent_ops[-1] - recent_ops[0]) / len(recent_ops)
                    
                    # Predictive scaling
                    if ops_trend > 100:  # Operations increasing rapidly
                        self.logger.info("🔮 Predicted high load - preemptive optimization")
                        await self._preemptive_optimization()
                    elif ops_trend < -50:  # Operations decreasing
                        self.logger.info("🔮 Predicted low load - resource conservation")
                        await self._resource_conservation()

        except Exception as e:
            self.logger.error(f"Predictive cache management error: {e}")

    async def _preemptive_optimization(self):
        """🚀 Preemptive optimization for predicted high load"""
        try:
            # Increase cache sizes
            self._l1_max_size = min(self._l1_max_size * 1.2, 25000)
            self._l2_max_size = min(self._l2_max_size * 1.2, 50000)
            
            # Extend TTL for better retention
            self._default_ttl = min(self._default_ttl * 1.2, 7200)
            
            # Prepare connection pool
            if self._pool_size < 15:
                await self._expand_connection_pool()
                
            self.logger.info("🚀 Applied preemptive optimizations for predicted high load")
            
        except Exception as e:
            self.logger.error(f"Preemptive optimization error: {e}")

    async def _resource_conservation(self):
        """🚀 Resource conservation for predicted low load"""
        try:
            # Light cleanup to free resources
            await self._optimize_cache_memory()
            
            # Reduce cache sizes slightly if they're large
            if self._l1_max_size > 15000:
                self._l1_max_size = max(self._l1_max_size * 0.9, 10000)
            
            self.logger.info("🌱 Applied resource conservation for predicted low load")
            
        except Exception as e:
            self.logger.error(f"Resource conservation error: {e}")

    async def _update_performance_metrics(self):
        """Update performance metrics for dashboard"""
        try:
            # Get current system metrics
            process = psutil.Process() if USE_PSUTIL else None
            memory_usage = process.memory_info().rss / 1024 / 1024 if process else 0
            
            # Update metrics in the main metrics collection task
            self.metrics.memory_usage_mb = memory_usage
            self.metrics.connections_active = self._active_connections

                # Log enhanced metrics
                self.logger.info("� DATABASE ULTIMATE PERFORMANCE:")
                self.logger.info(f"   💾 Cache Hit Rate: {hit_rate:.1f}%")
                self.logger.info(f"   🗄️  L1 Cache: {l1_size} entries, {memory_usage:.1f}MB")
                self.logger.info(f"   🗜️  L2 Cache: {l2_size} entries")
                self.logger.info(f"   ⚡ Avg Query Time: {self.metrics.avg_query_time*1000:.1f}ms")
                self.logger.info(f"   🔗 Active Connections: {self._active_connections}")
                self.logger.info(f"   📊 Total Operations: {total_ops:,}")
                self.logger.info(f"   🎯 Performance Score: {min(100, hit_rate + (100 - memory_usage)):.1f}/100")

                # Reset counters periodically to prevent overflow
                if total_ops > 10000:
                    self.metrics.cache_hits = int(self.metrics.cache_hits * 0.9)
                    self.metrics.cache_misses = int(self.metrics.cache_misses * 0.9)

            except Exception as e:
                self.logger.error(f"Enhanced metrics collection error: {e}")

    async def _preload_hot_data(self):
        """Preload frequently accessed data into cache"""
        try:
            conn = await self._get_connection()
            try:
                # Load most accessed data
                cursor = await conn.execute(
                    """
                    SELECT key, value, data_type, compressed, expires_at 
                    FROM data_store 
                    WHERE access_count > 5 
                    AND (expires_at IS NULL OR expires_at > ?)
                    ORDER BY access_count DESC 
                    LIMIT 100
                """,
                    (datetime.now(timezone.utc).isoformat(),),
                )

                rows = await cursor.fetchall()

                for row in rows:
                    key, value_blob, data_type, compressed, expires_at = row

                    # Decompress if needed
                    if compressed and USE_LZ4_COMPRESSION:
                        value = lz4.decompress(value_blob)
                    else:
                        value = value_blob

                    # Deserialize
                    if USE_FAST_JSON:
                        data = fast_json.loads(value)
                    else:
                        data = fast_json.loads(value.decode())

                    # Add to L1 cache
                    expires = datetime.fromisoformat(expires_at) if expires_at else None
                    entry = CacheEntry(
                        key=key,
                        value=data,
                        created_at=datetime.now(timezone.utc),
                        expires_at=expires,
                        data_type=DataType(data_type),
                        size_bytes=len(str(data).encode()),
                    )

                    self._l1_cache[key] = entry

                self.logger.info(f"🔥 Preloaded {len(rows)} hot data entries")

            finally:
                await self._return_connection(conn)

        except Exception as e:
            self.logger.error(f"Hot data preload error: {e}")

    async def _move_to_l2_cache(self, key: str, entry: CacheEntry):
        """Move entry from L1 to L2 with compression"""
        try:
            # Serialize
            if USE_FAST_JSON:
                serialized = fast_json.dumps(entry.value)
            else:
                serialized = fast_json.dumps(entry.value).encode()

            # Compress if beneficial
            if USE_LZ4_COMPRESSION and len(serialized) > 100:
                compressed = lz4.compress(serialized)
                self._l2_cache[key] = compressed
            else:
                self._l2_cache[key] = serialized

        except Exception as e:
            self.logger.error(f"L1 to L2 move error: {e}")

    async def get(
        self, key: str, default: Any = None, data_type: DataType = DataType.TEMPORARY
    ) -> Any:
        """
        Get value with multi-tier cache lookup

        Lookup order: L1 -> L2 -> L3 -> Database
        """
        start_time = time.time()

        try:
            # L1 Cache (memory)
            if key in self._l1_cache:
                entry = self._l1_cache[key]

                # Check expiry
                if entry.expires_at and datetime.now(timezone.utc) > entry.expires_at:
                    del self._l1_cache[key]
                else:
                    entry.access_count += 1
                    entry.last_accessed = datetime.now(timezone.utc)
                    self.metrics.cache_hits += 1
                    return entry.value

            # L2 Cache (compressed memory)
            if key in self._l2_cache:
                compressed_data = self._l2_cache[key]

                try:
                    # Decompress and deserialize
                    if USE_LZ4_COMPRESSION:
                        try:
                            decompressed = lz4.decompress(compressed_data)
                        except:
                            decompressed = compressed_data  # Not compressed
                    else:
                        decompressed = compressed_data

                    if USE_FAST_JSON:
                        value = fast_json.loads(decompressed)
                    else:
                        value = fast_json.loads(decompressed.decode())

                    # Promote to L1
                    entry = CacheEntry(
                        key=key,
                        value=value,
                        created_at=datetime.now(timezone.utc),
                        expires_at=None,  # Will be set from DB if needed
                        data_type=data_type,
                        size_bytes=len(str(value).encode()),
                    )

                    self._l1_cache[key] = entry
                    self.metrics.cache_hits += 1

                    return value

                except Exception as e:
                    self.logger.error(f"L2 decompression error: {e}")
                    del self._l2_cache[key]

            # L3 Cache (disk)
            l3_path = (
                self._l3_cache_path / f"{hashlib.md5(key.encode()).hexdigest()}.cache"
            )
            if l3_path.exists():
                try:
                    with open(l3_path, "rb") as f:
                        cached_data = f.read()

                    # Check if compressed
                    if USE_LZ4_COMPRESSION:
                        try:
                            decompressed = lz4.decompress(cached_data)
                        except:
                            decompressed = cached_data
                    else:
                        decompressed = cached_data

                    if USE_FAST_JSON:
                        value = fast_json.loads(decompressed)
                    else:
                        value = fast_json.loads(decompressed.decode())

                    # Promote to L1
                    entry = CacheEntry(
                        key=key,
                        value=value,
                        created_at=datetime.now(timezone.utc),
                        expires_at=None,
                        data_type=data_type,
                        size_bytes=len(str(value).encode()),
                    )

                    self._l1_cache[key] = entry
                    self.metrics.cache_hits += 1

                    return value

                except Exception as e:
                    self.logger.error(f"L3 cache read error: {e}")
                    l3_path.unlink(missing_ok=True)

            # Database lookup
            conn = await self._get_connection()
            try:
                cursor = await conn.execute(
                    "SELECT value, compressed, expires_at FROM data_store WHERE key = ?",
                    (key,),
                )
                row = await cursor.fetchone()

                if row:
                    value_blob, compressed, expires_at = row

                    # Check expiry
                    if expires_at:
                        expires = datetime.fromisoformat(expires_at)
                        if datetime.now(timezone.utc) > expires:
                            await conn.execute(
                                "DELETE FROM data_store WHERE key = ?", (key,)
                            )
                            await conn.commit()
                            self.metrics.cache_misses += 1
                            return default

                    # Update access count
                    await conn.execute(
                        "UPDATE data_store SET access_count = access_count + 1 WHERE key = ?",
                        (key,),
                    )
                    await conn.commit()

                    # Decompress if needed
                    if compressed and USE_LZ4_COMPRESSION:
                        decompressed = lz4.decompress(value_blob)
                    else:
                        decompressed = value_blob

                    # Deserialize
                    if USE_FAST_JSON:
                        value = fast_json.loads(decompressed)
                    else:
                        value = fast_json.loads(decompressed.decode())

                    # Add to cache
                    expires = datetime.fromisoformat(expires_at) if expires_at else None
                    entry = CacheEntry(
                        key=key,
                        value=value,
                        created_at=datetime.now(timezone.utc),
                        expires_at=expires,
                        data_type=data_type,
                        size_bytes=len(str(value).encode()),
                    )

                    self._l1_cache[key] = entry
                    self.metrics.cache_hits += 1

                    return value

                else:
                    self.metrics.cache_misses += 1
                    return default

            finally:
                await self._return_connection(conn)

        except Exception as e:
            self.logger.error(f"Get operation error: {e}")
            self.metrics.cache_misses += 1
            return default

        finally:
            # Update query time metrics
            query_time = time.time() - start_time
            self._query_times.append(query_time)
            if len(self._query_times) > 1000:
                self._query_times = self._query_times[-1000:]
            self.metrics.avg_query_time = sum(self._query_times) / len(
                self._query_times
            )

    async def set(
        self,
        key: str,
        value: Any,
        ttl_seconds: Optional[int] = None,
        data_type: DataType = DataType.TEMPORARY,
    ) -> bool:
        """
        Set value with intelligent caching
        """
        start_time = time.time()

        try:
            # Get data type configuration
            config = self._data_type_configs[data_type]
            effective_ttl = ttl_seconds or config["ttl_seconds"]

            # Calculate expiry
            expires_at = None
            if effective_ttl:
                expires_at = datetime.now(timezone.utc) + timedelta(
                    seconds=effective_ttl
                )

            # Serialize
            if USE_FAST_JSON:
                serialized = fast_json.dumps(value)
            else:
                serialized = fast_json.dumps(value).encode()

            # Compress if configured and beneficial
            compressed_data = serialized
            is_compressed = False

            if config["compress"] and USE_LZ4_COMPRESSION and len(serialized) > 100:
                compressed_data = lz4.compress(serialized)
                is_compressed = True

                # Update compression ratio metric
                if len(serialized) > 0:
                    ratio = len(compressed_data) / len(serialized)
                    self.metrics.compression_ratio = (
                        self.metrics.compression_ratio * 0.9
                    ) + (ratio * 0.1)

            # Store in database
            conn = await self._get_connection()
            try:
                await conn.execute(
                    """
                    INSERT OR REPLACE INTO data_store 
                    (key, value, data_type, expires_at, compressed, size_bytes, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                    (
                        key,
                        compressed_data,
                        data_type.value,
                        expires_at.isoformat() if expires_at else None,
                        is_compressed,
                        len(compressed_data),
                        datetime.now(timezone.utc).isoformat(),
                    ),
                )

                await conn.commit()

            finally:
                await self._return_connection(conn)

            # Add to appropriate cache level
            cache_level = config["cache_level"]

            if cache_level == CacheLevel.L1_MEMORY:
                # Add to L1
                entry = CacheEntry(
                    key=key,
                    value=value,
                    created_at=datetime.now(timezone.utc),
                    expires_at=expires_at,
                    data_type=data_type,
                    size_bytes=len(str(value).encode()),
                    compressed=is_compressed,
                )
                self._l1_cache[key] = entry

            elif cache_level == CacheLevel.L2_COMPRESSED:
                # Add to L2
                self._l2_cache[key] = compressed_data

            elif cache_level == CacheLevel.L3_DISK:
                # Add to L3
                l3_path = (
                    self._l3_cache_path
                    / f"{hashlib.md5(key.encode()).hexdigest()}.cache"
                )
                with open(l3_path, "wb") as f:
                    f.write(compressed_data)

            self.metrics.queries_executed += 1
            return True

        except Exception as e:
            self.logger.error(f"Set operation error: {e}")
            return False

        finally:
            query_time = time.time() - start_time
            self._query_times.append(query_time)
            if len(self._query_times) > 1000:
                self._query_times = self._query_times[-1000:]
            self.metrics.avg_query_time = sum(self._query_times) / len(
                self._query_times
            )

    async def delete(self, key: str) -> bool:
        """Delete value from all cache levels and database"""
        try:
            # Remove from caches
            self._l1_cache.pop(key, None)
            self._l2_cache.pop(key, None)

            # Remove from L3
            l3_path = (
                self._l3_cache_path / f"{hashlib.md5(key.encode()).hexdigest()}.cache"
            )
            l3_path.unlink(missing_ok=True)

            # Remove from database
            conn = await self._get_connection()
            try:
                cursor = await conn.execute(
                    "DELETE FROM data_store WHERE key = ?", (key,)
                )
                await conn.commit()
                return cursor.rowcount > 0
            finally:
                await self._return_connection(conn)

        except Exception as e:
            self.logger.error(f"Delete operation error: {e}")
            return False

    async def batch_get(
        self, keys: List[str], data_type: DataType = DataType.TEMPORARY
    ) -> Dict[str, Any]:
        """Optimized batch get operation"""
        results = {}
        missing_keys = []

        # Check L1 cache first
        for key in keys:
            if key in self._l1_cache:
                entry = self._l1_cache[key]
                if (
                    not entry.expires_at
                    or datetime.now(timezone.utc) <= entry.expires_at
                ):
                    results[key] = entry.value
                    entry.access_count += 1
                    entry.last_accessed = datetime.now(timezone.utc)
                    self.metrics.cache_hits += 1
                else:
                    del self._l1_cache[key]
                    missing_keys.append(key)
            else:
                missing_keys.append(key)

        # Batch database lookup for missing keys
        if missing_keys:
            conn = await self._get_connection()
            try:
                placeholders = ",".join("?" * len(missing_keys))
                cursor = await conn.execute(
                    f"""
                    SELECT key, value, compressed, expires_at 
                    FROM data_store 
                    WHERE key IN ({placeholders})
                """,
                    missing_keys,
                )

                rows = await cursor.fetchall()

                for row in rows:
                    key, value_blob, compressed, expires_at = row

                    # Check expiry
                    if expires_at:
                        expires = datetime.fromisoformat(expires_at)
                        if datetime.now(timezone.utc) > expires:
                            continue

                    # Decompress and deserialize
                    if compressed and USE_LZ4_COMPRESSION:
                        decompressed = lz4.decompress(value_blob)
                    else:
                        decompressed = value_blob

                    if USE_FAST_JSON:
                        value = fast_json.loads(decompressed)
                    else:
                        value = fast_json.loads(decompressed.decode())

                    results[key] = value
                    self.metrics.cache_hits += 1

                    # Add to L1 cache
                    expires = datetime.fromisoformat(expires_at) if expires_at else None
                    entry = CacheEntry(
                        key=key,
                        value=value,
                        created_at=datetime.now(timezone.utc),
                        expires_at=expires,
                        data_type=data_type,
                        size_bytes=len(str(value).encode()),
                    )
                    self._l1_cache[key] = entry

                # Update access counts
                found_keys = [row[0] for row in rows]
                if found_keys:
                    placeholders = ",".join("?" * len(found_keys))
                    await conn.execute(
                        f"""
                        UPDATE data_store 
                        SET access_count = access_count + 1 
                        WHERE key IN ({placeholders})
                    """,
                        found_keys,
                    )
                    await conn.commit()

            finally:
                await self._return_connection(conn)

            # Count cache misses
            self.metrics.cache_misses += len(missing_keys) - len(
                [key for key in missing_keys if key in results]
            )

        return results

    async def batch_set(
        self,
        data: Dict[str, Any],
        ttl_seconds: Optional[int] = None,
        data_type: DataType = DataType.TEMPORARY,
    ) -> bool:
        """Optimized batch set operation"""
        try:
            config = self._data_type_configs[data_type]
            effective_ttl = ttl_seconds or config["ttl_seconds"]

            expires_at = None
            if effective_ttl:
                expires_at = datetime.now(timezone.utc) + timedelta(
                    seconds=effective_ttl
                )

            # Prepare batch data
            batch_data = []
            for key, value in data.items():
                # Serialize
                if USE_FAST_JSON:
                    serialized = fast_json.dumps(value)
                else:
                    serialized = fast_json.dumps(value).encode()

                # Compress if configured
                compressed_data = serialized
                is_compressed = False

                if config["compress"] and USE_LZ4_COMPRESSION and len(serialized) > 100:
                    compressed_data = lz4.compress(serialized)
                    is_compressed = True

                batch_data.append(
                    (
                        key,
                        compressed_data,
                        data_type.value,
                        expires_at.isoformat() if expires_at else None,
                        is_compressed,
                        len(compressed_data),
                        datetime.now(timezone.utc).isoformat(),
                    )
                )

                # Add to appropriate cache
                if config["cache_level"] == CacheLevel.L1_MEMORY:
                    entry = CacheEntry(
                        key=key,
                        value=value,
                        created_at=datetime.now(timezone.utc),
                        expires_at=expires_at,
                        data_type=data_type,
                        size_bytes=len(str(value).encode()),
                        compressed=is_compressed,
                    )
                    self._l1_cache[key] = entry

            # Batch database insert
            conn = await self._get_connection()
            try:
                await conn.executemany(
                    """
                    INSERT OR REPLACE INTO data_store 
                    (key, value, data_type, expires_at, compressed, size_bytes, updated_at)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """,
                    batch_data,
                )

                await conn.commit()

            finally:
                await self._return_connection(conn)

            self.metrics.queries_executed += len(data)
            return True

        except Exception as e:
            self.logger.error(f"Batch set error: {e}")
            return False

    async def get_performance_stats(self) -> Dict[str, Any]:
        """Get comprehensive performance statistics"""
        l1_size = len(self._l1_cache)
        l2_size = len(self._l2_cache)
        l3_files = (
            len(list(self._l3_cache_path.glob("*.cache")))
            if self._l3_cache_path.exists()
            else 0
        )

        memory_usage = sum(entry.size_bytes for entry in self._l1_cache.values()) / (
            1024 * 1024
        )

        total_ops = self.metrics.cache_hits + self.metrics.cache_misses
        hit_rate = (self.metrics.cache_hits / max(total_ops, 1)) * 100

        return {
            "cache_stats": {
                "hit_rate_percent": hit_rate,
                "l1_entries": l1_size,
                "l2_entries": l2_size,
                "l3_entries": l3_files,
                "memory_usage_mb": memory_usage,
            },
            "database_stats": {
                "queries_executed": self.metrics.queries_executed,
                "avg_query_time_ms": self.metrics.avg_query_time * 1000,
                "active_connections": self._active_connections,
                "compression_ratio": self.metrics.compression_ratio,
            },
            "system_stats": {
                "initialized": self._initialized,
                "total_cache_operations": total_ops,
                "cache_hits": self.metrics.cache_hits,
                "cache_misses": self.metrics.cache_misses,
            },
        }

    async def cleanup(self):
        """Cleanup database resources"""
        try:
            self.logger.info("🧹 Starting database cleanup...")

            self._initialized = False

            # Cancel background tasks
            if self._cleanup_task:
                self._cleanup_task.cancel()
            if self._metrics_task:
                self._metrics_task.cancel()

            # Close all connections
            async with self._pool_lock:
                for conn in self._connection_pool:
                    await conn.close()
                self._connection_pool.clear()

            self.logger.info("✅ Database cleanup completed")

        except Exception as e:
            self.logger.error(f"Database cleanup error: {e}")


# Global instance
_database: Optional[UltraPerformanceDatabase] = None


async def get_ultra_database(
    db_path: str = "data/astra_optimized.db",
) -> UltraPerformanceDatabase:
    """Get or create the global database instance"""
    global _database
    if _database is None:
        _database = UltraPerformanceDatabase(db_path)
        await _database.initialize()
    return _database


async def initialize_ultra_database(
    db_path: str = "data/astra_optimized.db",
) -> UltraPerformanceDatabase:
    """Initialize the global database instance"""
    global _database
    _database = UltraPerformanceDatabase(db_path)
    await _database.initialize()
    return _database


# Compatibility aliases
get_database = get_ultra_database
initialize_database = initialize_ultra_database
